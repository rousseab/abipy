{
 "metadata": {
  "name": "",
  "signature": "sha256:bd586c47ee95836f01b17e29a5a7851f631321f31f5ae4b95847228260fda393"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TaskManager"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `TaskManager` is responsible for the submission of the tasks (creation of the submission script, initialization of the shell environment) as well as for the optimization of the parameters used for the parallel runs (number of MPI processes, number of OpeMP threads, automatic parallelization with ABINIT `autoparal` feature). \n",
      "The configuration file for the `TaskManager` is written in YAML (a good introduction to the YAML syntax can be found at http://yaml.org/spec/1.1/#id857168).\n",
      "A typical example is reported below:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Resource manager e.g slurm, pbs, shell\n",
      "qtype: slurm\n",
      "# Options passed to the resource manager (syntax depends on qtype, consult the manual of your resource manager)\n",
      "qparams:\n",
      "    ntasks: 2\n",
      "    time: 0:20:00\n",
      "    partition: Oban\n",
      "# List of modules to import before running the calculation\n",
      "modules:\n",
      "    - intel/compilerpro/13.0.1.117\n",
      "    - fftw3/intel/3.3\n",
      "# Shell environment\n",
      "shell_env:\n",
      "     PATH: /home/user/local/bin/:$PATH\n",
      "     LD_LIBRARY_PATH: /home/user/local/lib:$LD_LIBRARY_PATH\n",
      "mpi_runner: /path/to/mpirun\n",
      "# Options for the automatic parallelization (Abinit autoparal feature)\n",
      "policy:\n",
      "    autoparal: 1\n",
      "    max_ncpus: 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`qtype` specifies the queue resource manager (Slurm in this example). `qparams` is a dictionary with the parameters \n",
      "passed to the resource manager. \n",
      "We use the *normalized* version of the options i.e dashes in the official name of the parameter are replaced by  underscores  (for the list of supported options see ...)\n",
      "`modules` is the list of modules to load, while `shell_env` allows the user to specify or to modfiy the values of the environment variables.\n",
      "The `policy` section governs the automatic parallelization of the run: in this case abipy will use the `autoparal` features of abinit to determine an optimal configuration with **maximum** `max_ncpus` MPI nodes. Setting autoparal to 0 disables the automatic parallelization. **Other values of autoparal are not supported**.\n",
      "One can put this configuration file either in the configuration directory `$HOME/.abinit/abipy` or in the current working directory (the latter has precedence over the global configuration file located in `$HOME/.abinit/abipy`).\n",
      "The `TaskManager` can then be easily initialized by calling the class method `from_user_config`  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from abipy import abilab \n",
      "manager = abilab.TaskManager.from_user_config()\n",
      "print(manager)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Task policy]\n",
        "autoparal: 0\n",
        "mode: default\n",
        "max_ncpus: None\n",
        "automemory: 0\n",
        "vars_condition: None\n",
        "condition: None\n",
        "[Qadapter 0]\n",
        "ShellAdapter:localhost\n",
        "Hardware:\n",
        "   num_nodes: 1, sockets_per_node: 1, cores_per_socket: 2, mem_per_node 4096,\n",
        "Qadapter selected: 0\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In some cases, you may want to enforce some constraint on the \"optimal\" configuration. For example, you may want to select only those configurations whose parallel efficiency is greater than 0.7 and whose number of MPI nodes is divisible\n",
      "by 4. One can easily enforce this constraint via the `condition` dictionary whose syntax is similar to the one used in `mongodb`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "policy:\n",
      "    autoparal: 1\n",
      "    max_ncpus: 10\n",
      "    condition: {$and: [ {\"efficiency\": {$gt: 0.7}}, {\"tot_ncpus\": {$divisible: 4}} ]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-2-3676d28891b7>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3676d28891b7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    policy:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parallel efficiency is defined as $\\epsilon = \\dfrac{T_1}{T_N * N}$ where $N$ is the number of MPI processes and $T_j$ is the wall time \n",
      "needed to complete the calculation with $j$ MPI processes. For a perfect scaling implementation $\\epsilon$ is equal to one.\n",
      "The parallel speedup with N processors is given by $S = T_N / T_1$.\n",
      "Note that `autoparal = 1` will automatically change your `job.sh` script as well as the input file so that we can run the job in parallel with the optimal configuration required by the user. For example, you can use `paral_kgb` in GS calculations and `abipy` will automatically set the values of `npband`, `npfft`, `npkpt` ... for you! \n",
      "Note that if no configuration fulfills the given condition, abipy will use the optimal configuration that leads to the highest parallel speedup (not necessarily the most efficient one)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}