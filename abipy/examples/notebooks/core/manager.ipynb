{
 "metadata": {
  "name": "",
  "signature": "sha256:d2838929776a257ede3ef8a13b00df7c31f2828d867e45d029aaaf723d4edc81"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TaskManager"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `TaskManager` is responsible for the submission of the tasks (creation of the submission script, initialization of the shell environment) as well as for the optimization of the parameters used for the parallel runs (number of MPI processes, number of OpeMP threads, automatic parallelization with ABINIT `autoparal` feature). \n",
      "\n",
      "The configuration file for the `TaskManager` is written in YAML (a good introduction to the YAML syntax can be found at http://yaml.org/spec/1.1/#id857168).\n",
      "See also http://www.yaml.org/refcard.html for the YAML reference card.\n",
      "A typical example is reported below:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Resource manager e.g slurm, pbs, shell\n",
      "qtype: slurm\n",
      "# Options passed to the resource manager (syntax depends on qtype, consult the manual of your resource manager)\n",
      "qparams:\n",
      "    ntasks: 2\n",
      "    time: 0:20:00\n",
      "    partition: Oban\n",
      "# List of modules to import before running the calculation\n",
      "modules:\n",
      "    - intel/compilerpro/13.0.1.117\n",
      "    - fftw3/intel/3.3\n",
      "# Shell environment\n",
      "shell_env:\n",
      "     PATH: /home/user/local/bin/:$PATH\n",
      "     LD_LIBRARY_PATH: /home/user/local/lib:$LD_LIBRARY_PATH\n",
      "mpi_runner: /path/to/mpirun\n",
      "# Options for the automatic parallelization (Abinit autoparal feature)\n",
      "policy:\n",
      "    autoparal: 1\n",
      "    max_ncpus: 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`qtype` specifies the queue resource manager (Slurm in this example). `qparams` is a dictionary with the parameters \n",
      "passed to the resource manager. \n",
      "We use the *normalized* version of the options i.e dashes in the official name of the parameter are replaced by  underscores  (for the list of supported options see ...)\n",
      "`modules` is the list of modules to load, while `shell_env` allows the user to specify or to modfiy the values of the environment variables.\n",
      "The `policy` section governs the automatic parallelization of the run: in this case abipy will use the `autoparal` features of abinit to determine an optimal configuration with **maximum** `max_ncpus` MPI nodes. Setting autoparal to 0 disables the automatic parallelization. **Other values of autoparal are not supported**.\n",
      "One can put this configuration file either in the configuration directory `$HOME/.abinit/abipy` or in the current working directory (the latter has precedence over the global configuration file located in `$HOME/.abinit/abipy`).\n",
      "The `TaskManager` can then be easily initialized by calling the class method `from_user_config`  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from abipy import abilab \n",
      "manager = abilab.TaskManager.from_user_config()\n",
      "print(manager)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Qadapter 0]\n",
        "ShellAdapter:gmac\n",
        "Hardware:\n",
        "   num_nodes: 1, sockets_per_node: 1, cores_per_socket: 2, mem_per_node 4096,\n",
        "Qadapter selected: 0\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In some cases, you may want to enforce some constraint on the \"optimal\" configuration. For example, you may want to select only those configurations whose parallel efficiency is greater than 0.7 and whose number of MPI nodes is divisible\n",
      "by 4. One can easily enforce this constraint via the `condition` dictionary whose syntax is similar to the one used in `mongodb`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```json\n",
      "policy:\n",
      "    autoparal: 1\n",
      "    max_ncpus: 10\n",
      "    condition: {$and: [ {\"efficiency\": {$gt: 0.7}}, {\"tot_ncpus\": {$divisible: 4}} ]}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parallel efficiency is defined as $\\epsilon = \\dfrac{T_1}{T_N * N}$ where $N$ is the number of MPI processes and $T_j$ is the wall time \n",
      "needed to complete the calculation with $j$ MPI processes. For a perfect scaling implementation $\\epsilon$ is equal to one.\n",
      "The parallel speedup with N processors is given by $S = T_N / T_1$.\n",
      "Note that `autoparal = 1` will automatically change your `job.sh` script as well as the input file so that we can run the job in parallel with the optimal configuration required by the user. For example, you can use `paral_kgb` in GS calculations and `abipy` will automatically set the values of `npband`, `npfft`, `npkpt` ... for you! \n",
      "Note that if no configuration fulfills the given condition, abipy will use the optimal configuration that leads to the highest parallel speedup (not necessarily the most efficient one)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The complete list of options supported by the `TaskManager` with slurm can be \n",
      "retrieved with the command:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!abirun.py . docmanager slurm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "# TaskManager configuration file (YAML Format)\r\n",
        "# Main options:\r\n",
        "\r\n",
        "policy: \r\n",
        "\r\n",
        "qadapters:  \r\n",
        "    # List of qadapters objects (mandatory)\r\n",
        "    -  # qadapter_1\r\n",
        "    -  # qadapter_2\r\n",
        "\r\n",
        "db_connector: # Connection to MongoDB database (optional)\r\n",
        "\r\n",
        "##########################################\r\n",
        "# Individual entries are documented below:\r\n",
        "##########################################\r\n",
        "\r\n",
        "qadapter: \r\n",
        "# dictionary with info on the hardware available on this particular queue.\r\n",
        "hardware:  \r\n",
        "    num_nodes:        # Number of nodes available on this queue. Mandatory\r\n",
        "    sockets_per_node: # Self-explanatory. Mandatory.\r\n",
        "    cores_per_socket: # Self-explanatory. Mandatory.\r\n",
        "\r\n",
        "# dictionary with the options used to prepare the enviroment before submitting the job\r\n",
        "job:\r\n",
        "    setup:       # List of commands (str) executed before running (default empty)\r\n",
        "    omp_env:     # Dictionary with OpenMP env variables (default empty i.e. no OpenMP)\r\n",
        "    modules:     # List of modules to be imported (default empty)\r\n",
        "    shell_env:   # Dictionary with shell env variables.\r\n",
        "    mpi_runner:  # MPI runner i.e. mpirun, mpiexec, Default is None i.e. no mpirunner\r\n",
        "    pre_run:     # List of commands executed before the run (default: empty)\r\n",
        "    post_run:    # List of commands executed after the run (default: empty)\r\n",
        "\r\n",
        "# dictionary with the name of the queue and optional parameters \r\n",
        "# used to build/customize the header of the submission script.\r\n",
        "queue:\r\n",
        "    qname:   # Name of the queue (mandatory)\r\n",
        "    qparams: # Dictionary with values used to generate the header of the job script\r\n",
        "             # See pymatgen.io.abinitio.qadapters.py for the list of supported values.\r\n",
        "\r\n",
        "# dictionary with the constraints that must be fulfilled in order to run on this queue.\r\n",
        "limits:\r\n",
        "    min_cores:         # Minimum number of cores (default 1)\r\n",
        "    max_cores:         # Maximum number of cores (mandatory)\r\n",
        "    min_mem_per_proc:  # Minimum memory per MPI process in Mb, units can be specified e.g. 1.4 Gb\r\n",
        "                       # (default hardware.mem_per_core)\r\n",
        "    max_mem_per_proc:  # Maximum memory per MPI process in Mb, units can be specified e.g. `1.4Gb`\r\n",
        "                       # (default hardware.mem_per_node)\r\n",
        "    condition:         # MongoDB-like condition (default empty, i.e. not used)\r\n",
        "\r\n",
        "db_connector: \r\n",
        "     enabled:     # yes or no (default yes)\r\n",
        "     database:    # Name of the mongodb database (default abinit)\r\n",
        "     collection:  # Name of the collection (default test)\r\n",
        "     host:        # host address e.g. 0.0.0.0 (default None)\r\n",
        "     port:        # port e.g. 8080 (default None)\r\n",
        "     user:        # user name (default None)\r\n",
        "     password:    # password for authentication (default None)\r\n",
        "     \r\n",
        "qtype supported: [u'shell', u'slurm', u'pbspro', u'sge', u'moab', u'torque']\r\n",
        "Use `abirun.py . docmanager slurm` to have the list of qparams for slurm.\r\n",
        "\r\n",
        "QPARAMS for slurm\r\n",
        "#!/bin/bash\r\n",
        "\r\n",
        "#SBATCH --partition=$${partition}\r\n",
        "#SBATCH --job-name=$${job_name}\r\n",
        "#SBATCH --nodes=$${nodes}\r\n",
        "#SBATCH --total_tasks=$${total_tasks}\r\n",
        "#SBATCH --ntasks=$${ntasks}\r\n",
        "#SBATCH --ntasks-per-node=$${ntasks_per_node}\r\n",
        "#SBATCH --cpus-per-task=$${cpus_per_task}\r\n",
        "#SBATCH --mem=$${mem}\r\n",
        "#SBATCH --mem-per-cpu=$${mem_per_cpu}\r\n",
        "#SBATCH --hint=$${hint}\r\n",
        "#SBATCH --time=$${time}\r\n",
        "#SBATCH\t--exclude=$${exclude_nodes}\r\n",
        "#SBATCH --account=$${account}\r\n",
        "#SBATCH --mail-user=$${mail_user}\r\n",
        "#SBATCH --mail-type=$${mail_type}\r\n",
        "#SBATCH --constraint=$${constraint}\r\n",
        "#SBATCH --gres=$${gres}\r\n",
        "#SBATCH --requeue=$${requeue}\r\n",
        "#SBATCH --nodelist=$${nodelist}\r\n",
        "#SBATCH --propagate=$${propagate}\r\n",
        "#SBATCH --licenses=$${licenses}\r\n",
        "#SBATCH --output=$${_qout_path}\r\n",
        "#SBATCH --error=$${_qerr_path}\r\n",
        "$${qverbatim}\r\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}